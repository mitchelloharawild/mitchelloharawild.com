{
  "hash": "3be41fcc7d94f18dc6b701da9691db8d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Introducing taipan\ndescription: 'Annotate images for analysis with taipan'\ndate: '2018-09-27'\ncategories:\n  - release\n  - image\n  - machine learning\n  - shiny\ntags:\n  - package\n  - cran\n  - shiny\nimage: 'taipan_app.png'\nimage-alt: 'Default taipan app'\n---\n\n\n\n\n`<img src=\"taipan.png\" class=\"hex\"/>`{=html} \n\nAnnotating images is tiresome work, and existing tools do not make this much easier. Identifying features within images is a common task for training and evaluating machine learning models, and taipan aims to simplify this very manual process.\n\n# What is taipan?\n\n*taipan* is a **T**ool for **A**nnotating **I**mages in **P**reparation for **AN**alysis. It provides a customisable shiny app that pairs image area selection with a set of shiny inputs to flexibly classify the contents of images. Unlike most shiny app packages, taipan provides functionality to customise the key components (images and questions) of the app, and dynamically builds an app ready for deployment and sharing.\n\nThe package originated from a research project with Tennis Australia, where the training dataset of 6406 images of tennis broadcast images were painstakingly annotated by Stephanie Kobakian ([\\@srkobakian](https://twitter.com/srkobakian)). The package is the result of many iterations of the app we created to annotate these images.\n\n# Getting started with taipan\n\nThe taipan package is now available on CRAN, so it can be easily installed using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"taipan\")\n```\n:::\n\n\n*taipan* provides two key functions that are used to build your own image annotation app, `taipanQuestions` is used to build a set of questions, and `buildTaipan` combines your questions and images to build an app in your folder of choice.\n\nThese lists of questions can be flexibly produced using the `taipanQuestions` function, where any shiny inputs and web elements can be used to build your own survey for 'scene' and 'selection' scenarios. Scene questions are suitable for questions that apply to the whole image, and are shown when no selection is made. Selection questions are appropriate for selected areas of the image, and are shown when a selection is made.\n\nThe questions can then be used to produce the app with `buildTaipan`, with a set images can be provided using local files and links to images online.\n\n# Example: Not hotdog\n\nSuppose we're interested in training a model to identify hotdogs in an image. To do this, we require a training dataset that describes the location and features of the hotdog.\n\n![](nothotdog.gif)\n\nWe would expect a few features that would useful for training the model, such as the existence of a hotdog, condiments of the hotdog, and overall quality of the image. Using shiny, we can construct this question interface using a variety of inputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(taipan)\nlibrary(shiny)\nquestions <- taipanQuestions(\n  scene = sliderInput(\n    \"quality\", label = \"Image Quality\",\n    min = 0, max = 10, value = 5),\n  selection = div(\n    radioButtons(\"hotdog\", label = \"Hotdog?\",\n      choices = list(\"Hotdog\", \"Not hotdog\")),\n    checkboxGroupInput(\"extra\", label = \"Condiments\",\n      choices = list(\"Onion\", \"Tomato (Ketchup)\", \"Barbeque\", \"Mustard\"))\n  )\n)\n```\n:::\n\n\nNext, we need to find a set of questions to use. For this example, I've provided two sample images on the package's GitHub repository.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimages <- c(\"https://raw.githubusercontent.com/srkobakian/taipan/master/sample_images/hotdog.jpg\",\n            \"https://raw.githubusercontent.com/srkobakian/taipan/master/sample_images/not_hotdog.jpg\")\n```\n:::\n\n\nFinally, we can build our app using these questions and images.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuildTaipan(questions, images, appdir = \"~/Shiny Applications/nothotdog\")\n```\n:::\n\n\n![](hotdog_app.png)\n\nPreview this app at [shiny.mitchelloharawild.com/nothotdog/](https://shiny.mitchelloharawild.com/nothotdog/), or run the code to build your own copy.\n\nOnce the images have been annotated, the 'Export Responses' button can be used to download the data. The data is provided in a long tidy format, where the responses to scene and selection questions are merged and ready for model training and analysis\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|image_name     | quality| xmin| xmax| ymin|  ymax|hotdog |extra                            |\n|:--------------|-------:|----:|----:|----:|-----:|:------|:--------------------------------|\n|hotdog.jpg     |       6|   51|  249| 36.5| 303.5|Hotdog |Onion, Tomato (Ketchup), Mustard |\n|hotdog.jpg     |       6|  272|  486| 32.5| 282.5|Hotdog |Onion, Tomato (Ketchup), Mustard |\n|not_hotdog.jpg |       7|   NA|   NA|   NA|    NA|NA     |NA                               |\n\n\n:::\n:::\n\n\n# Additional resources\n\n* Stephanie's ([\\@srkobakian](https://twitter.com/srkobakian)) [useR!2018 lightning talk](https://srk.netlify.com/talks/taipan)\n* The vignette which shows [annotation of tennis images](https://srkobakian.github.io/taipan/articles/taipan-vignette.html)\n* The [GitHub repository](https://github.com/srkobakian/taipan)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}