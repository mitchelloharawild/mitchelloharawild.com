[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mitchell O’Hara-Wild",
    "section": "",
    "text": "PhD candidate at Monash University; Consultant at Nectric. Data scientist and developer of statistical software."
  },
  {
    "objectID": "blog/taipan/index.html",
    "href": "blog/taipan/index.html",
    "title": "Introducing taipan",
    "section": "",
    "text": "Annotating images is tiresome work, and existing tools do not make this much easier. Identifying features within images is a common task for training and evaluating machine learning models, and taipan aims to simplify this very manual process.\n\nWhat is taipan?\ntaipan is a Tool for Annotating Images in Preparation for ANalysis. It provides a customisable shiny app that pairs image area selection with a set of shiny inputs to flexibly classify the contents of images. Unlike most shiny app packages, taipan provides functionality to customise the key components (images and questions) of the app, and dynamically builds an app ready for deployment and sharing.\nThe package originated from a research project with Tennis Australia, where the training dataset of 6406 images of tennis broadcast images were painstakingly annotated by Stephanie Kobakian (@srkobakian). The package is the result of many iterations of the app we created to annotate these images.\n\n\nGetting started with taipan\nThe taipan package is now available on CRAN, so it can be easily installed using:\n\ninstall.packages(\"taipan\")\n\ntaipan provides two key functions that are used to build your own image annotation app, taipanQuestions is used to build a set of questions, and buildTaipan combines your questions and images to build an app in your folder of choice.\nThese lists of questions can be flexibly produced using the taipanQuestions function, where any shiny inputs and web elements can be used to build your own survey for ‘scene’ and ‘selection’ scenarios. Scene questions are suitable for questions that apply to the whole image, and are shown when no selection is made. Selection questions are appropriate for selected areas of the image, and are shown when a selection is made.\nThe questions can then be used to produce the app with buildTaipan, with a set images can be provided using local files and links to images online.\n\n\nExample: Not hotdog\nSuppose we’re interested in training a model to identify hotdogs in an image. To do this, we require a training dataset that describes the location and features of the hotdog.\n\nWe would expect a few features that would useful for training the model, such as the existence of a hotdog, condiments of the hotdog, and overall quality of the image. Using shiny, we can construct this question interface using a variety of inputs.\n\nlibrary(taipan)\nlibrary(shiny)\nquestions &lt;- taipanQuestions(\n  scene = sliderInput(\n    \"quality\", label = \"Image Quality\",\n    min = 0, max = 10, value = 5),\n  selection = div(\n    radioButtons(\"hotdog\", label = \"Hotdog?\",\n      choices = list(\"Hotdog\", \"Not hotdog\")),\n    checkboxGroupInput(\"extra\", label = \"Condiments\",\n      choices = list(\"Onion\", \"Tomato (Ketchup)\", \"Barbeque\", \"Mustard\"))\n  )\n)\n\nNext, we need to find a set of questions to use. For this example, I’ve provided two sample images on the package’s GitHub repository.\n\nimages &lt;- c(\"https://raw.githubusercontent.com/srkobakian/taipan/master/sample_images/hotdog.jpg\",\n            \"https://raw.githubusercontent.com/srkobakian/taipan/master/sample_images/not_hotdog.jpg\")\n\nFinally, we can build our app using these questions and images.\n\nbuildTaipan(questions, images, appdir = \"~/Shiny Applications/nothotdog\")\n\n\nPreview this app at shiny.mitchelloharawild.com/nothotdog/, or run the code to build your own copy.\nOnce the images have been annotated, the ‘Export Responses’ button can be used to download the data. The data is provided in a long tidy format, where the responses to scene and selection questions are merged and ready for model training and analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_name\nquality\nxmin\nxmax\nymin\nymax\nhotdog\nextra\n\n\n\n\nhotdog.jpg\n6\n51\n249\n36.5\n303.5\nHotdog\nOnion, Tomato (Ketchup), Mustard\n\n\nhotdog.jpg\n6\n272\n486\n32.5\n282.5\nHotdog\nOnion, Tomato (Ketchup), Mustard\n\n\nnot_hotdog.jpg\n7\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nAdditional resources\n\nStephanie’s (@srkobakian) useR!2018 lightning talk\nThe vignette which shows annotation of tennis images\nThe GitHub repository\n\n\n\n\n\nCitationBibTeX citation:@online{o'hara-wild2018,\n  author = {O’Hara-Wild, Mitchell},\n  title = {Introducing Taipan},\n  date = {2018-09-27},\n  url = {https://mitchelloharawild.com//blog/taipan},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nO’Hara-Wild, Mitchell. 2018. “Introducing Taipan.”\nSeptember 27, 2018. https://mitchelloharawild.com//blog/taipan."
  },
  {
    "objectID": "blog/vitae-0-2-0/index.html",
    "href": "blog/vitae-0-2-0/index.html",
    "title": "vitae 0.2.0",
    "section": "",
    "text": "r htmltools::img(src = \"vitae.png\", class = \"hex\")\nThe v0.2.0 update to the vitae package is now available on CRAN, and along with it comes with new templates and more features. Read on to learn more, or refer to the changelog for a brief summary.\n\nNew templates and themes\nThis release introduces 5 new styles ready for you to try with your vitae. Theme support for moderncv has been added, allowing you to choose between casual (existing default), classic, oldstyle, banking and fancy. Themes for moderncv are specified in the YAML header:\noutput: \n  vitae::moderncv:\n    theme: classic\nNewly added is the classic theme from the latexcv template is now available using the vitae::latexcv output format. Other themes from this template will be added in a future version (contributions welcomed).\nIf you’d like to contribute a new template to the vitae package, take a look at the creating vitae templates vignette.\n\n\nImprovements\nThis update includes a few incremental improvements. For those creating a new CV you’ll now start from a more complete example of a vitae that features usage of brief_entries(), detailed_entries() and bibliography_entries(). We hope that this will make it easier for you to see how each of these functions work, giving you a quick start to making your own résumé.\nWe’ve also added two new arguments for the YAML header. docname can be used to modify the “Curriculum Vitae” or “Résumé” text that exists in the template. We’ve also added surname, which allows special template formatting (output for the name remains unchanged).\nSpeaking of surnames, this field will now appear in the dataset returned by bibliography_entries(). This is particularly useful for the improvements made to sorting bibliography entries, as they will now respect the order from the data.\nLastly there is a new data sources for vitae vignette, which highlights a few useful approaches to getting data into your CV.\n\n\nBreaking changes\nUnfortunately we didn’t get the interface for bibliography_entries() exactly right in the first version. We have decided that the title and sorting are not required and have deprecated them.\nThe sorting argument was a common source of confusion for users that are unfamiliar the sorting interface for LaTeX’s biblatex. Instead, we now use the order of entries in the tibble returned by bibliography_entries(), which allows you to use dplyr::arrange() to re-order the entries to suit your needs. Instead of using the title argument, we now recommend that you use markdown headers.\n\n\nContributors\nThanks to contributions from @chrisumphlett, and @jonmcalder for this release. Contributions are welcomed, check out the GitHub repository for more details: https://github.com/ropenscilabs/vitae\n\n\n\n\nCitationBibTeX citation:@online{o'hara-wild2019,\n  author = {O’Hara-Wild, Mitchell},\n  title = {Vitae 0.2.0},\n  date = {2019-07-12},\n  url = {https://mitchelloharawild.com//blog/vitae-0-2-0},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nO’Hara-Wild, Mitchell. 2019. “Vitae 0.2.0.” July 12, 2019.\nhttps://mitchelloharawild.com//blog/vitae-0-2-0."
  },
  {
    "objectID": "blog/loading-r-packages-in-a-loop/index.html",
    "href": "blog/loading-r-packages-in-a-loop/index.html",
    "title": "Loading R packages in a loop",
    "section": "",
    "text": "In Nick Tierney (@nj_tierney) and Saskia Freytag’s (@trashystats) second Credibly Curious podcast, they briefly delve into the confusing world of non-standard evaluation (NSE). As part of this discussion, podcast guest Roger Peng (@rdpeng) noted that:\nAlthough not a pop-quiz, it is certainly a challenge, and a common cause of confusion for R users.\nMost R users would load packages using the library function, such as library(tidyverse). So to load packages in a loop, one might try:\npackages &lt;- c(\"ggplot2\", \"dplyr\")\nfor(pkg in packages){\n  library(pkg)\n}\n\n#&gt; Error in library(pkg): there is no package called 'pkg'\nIf it were that simple, it wouldn’t warrant a blog post! This doesn’t work because the library function uses non-standard evaluation. That is what allows you to use library(tidyverse) instead of library(\"tidyverse\"). In the loop, R tries to be helpful by loading pkg instead of the value stored inside (“ggplot2”, and then “dplyr”)."
  },
  {
    "objectID": "blog/loading-r-packages-in-a-loop/index.html#what-is-non-standard-evaluation",
    "href": "blog/loading-r-packages-in-a-loop/index.html#what-is-non-standard-evaluation",
    "title": "Loading R packages in a loop",
    "section": "What is non-standard evaluation?",
    "text": "What is non-standard evaluation?\nFor most R users, an understanding of non-standard evaluation (NSE) is rarely needed. You may not know what non-standard evaluation is, but you have definitely used it before (perhaps without even realising). In fact, NSE is used each time you load in a package without quoting the package name.\nMost tidyverse packages also leverage NSE to simplify the typing needed to transform a dataset or plot some data. Try to identify the NSE parts in the following code examples:\n\nlibrary(dplyr)\nmtcars %&gt;%\n  mutate(displ_l = disp / 61.0237)\n\n\nlibrary(ggplot2)\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point()\n\nSo what is non-standard evaluation? As the name may suggest, it is code which is evaluated in a non-standard way. As an example, let’s look at the dplyr code above. The mutate function is calculating disp / 61.0237 and saving the result as a column called displ_l. Standard evaluation in R would find the disp variable and compute the division, so let’s try that:\n\ndisp / 61.0237\n\n#&gt; Error in eval(expr, envir, enclos): object 'disp' not found\n\n\nR is unable to find the disp variable because it exists as a column in the mtcars dataset, not in the evaluation environment. When using this code in the mutate function, dplyr helpfully prevents evaluation, and later re-evaluates by first looking in the provided data, and then in the evaluation environment. So when this code is used in the mutate function, R is now able to find disp, because dplyr has changed where R looks for the variable.\n\nmtcars %&gt;%\n  mutate(displ_l = disp / 61.0237)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\ndispl_l\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n2.621932\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n2.621932\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n1.769804\n\n\n\n\n\nFor more details on non-standard evaluation, I recommend reading the Advanced R book."
  },
  {
    "objectID": "blog/loading-r-packages-in-a-loop/index.html#using-nse-to-load-packages-in-a-loop",
    "href": "blog/loading-r-packages-in-a-loop/index.html#using-nse-to-load-packages-in-a-loop",
    "title": "Loading R packages in a loop",
    "section": "Using NSE to load packages in a loop",
    "text": "Using NSE to load packages in a loop\nSo, now with a brief understanding of NSE, let’s try to use the library function in a loop again. Remember, the issue is that library uses non-standard evaluation on package names, so we can’t use library(pkg). Instead, we need to use NSE ourselves to substitute pkg with the name of the package itself, as if you had written it directly into the console. To achieve this, we need to build an expression, which is simply code which has not yet been evaluated.\nThere are many different ways to do this, but I will suggest two similar methods: one using base R, and one using the tidyverse.\nIn base, you can replace values in an expression using bquote() and .() to create the desired expression.\n\npkg &lt;- \"ggplot2\"\nbquote(library(.(pkg)))\n\n#&gt; library(\"ggplot2\")\n\n\nUsing rlang, we can achieve a similar result using expr() and !! to replace the pkg with the actual variable.\n\nlibrary(rlang)\npkg &lt;- \"ggplot2\"\nexpr(library(!!pkg))\n\n#&gt; library(\"ggplot2\")\n\n\nAll that is left is to evaluate these expressions using eval or eval_tidy in a loop, which will run the code and load the packages.\n\n# Base\nfor(pkg in c(\"ggplot2\", \"dplyr\")){\n  eval(bquote(library(.(pkg))))\n}\n\n# Tidy\nlibrary(purrr)\nlibrary(rlang)\nc(\"ggplot2\", \"dplyr\") %&gt;%\n  map(~ eval_tidy(expr(library(!!.x))))"
  },
  {
    "objectID": "blog/loading-r-packages-in-a-loop/index.html#alternatively",
    "href": "blog/loading-r-packages-in-a-loop/index.html#alternatively",
    "title": "Loading R packages in a loop",
    "section": "Alternatively…",
    "text": "Alternatively…\nYou could also set character.only = TRUE which prevents the use of non-standard evaluation. But if I started with that, I wouldn’t have a good excuse to talk about the wonders of non-standard evaluation!\n\n# With a for loop\nfor(pkg in c(\"ggplot2\", \"dplyr\")){\n  library(pkg, character.only = TRUE)\n}\n\n# Or with functional programming\nmap(c(\"ggplot2\", \"dplyr\"), library, character.only = TRUE)"
  },
  {
    "objectID": "blog/tsibbledata/index.html",
    "href": "blog/tsibbledata/index.html",
    "title": "Introducing tsibbledata",
    "section": "",
    "text": "The tsibbledata package provides a diverse collection of datasets for learning how to work with tidy time series data. There are 12 time series datasets included in the package, each of which featuring a unique time series characteristics or structures. These datasets are stored as tsibble objects, which allows time series data to be used with the tidyverse.\nThe tsibbledata package is now available on CRAN, so it can be easily installed using:\n\ninstall.packages(\"tsibbledata\")\n\n\nIncluded datasets\n\nansett contains more than 5 years weekly passenger numbers on Ansett Australia flights between pairs of Australian airports by ticket class. Ansett no longer in business, and the data features a major pilot strike and at least two definition changes for ticket classes.\nvic_elec provides 3 years of half hourly electricity demand data for Victoria, Australia. The data also includes key predictors of demand: temperature and holiday information. The data contains multiple seasonal patterns, and strong non-linear relationships with temperature.\naus_livestock details the monthly meat production in Australia for over 50 years. The data is disaggregated into 7 different groups of animals, for the 8 major states and territories of Australia.\naus_production features the quarterly production of selected commodities in Australia for over 50 years. The included commodities are coupled by types: Beer and Tobacco, Bricks and Cement, Electricity and Gas.\naus_retail contains monthly Turnover for 20 Australian industries between 1982 and 2018. The data is provided for the Australia’s 8 major states and territories. A wide variety of seasonal patterns and economic structures are evident in the 152 series.\ngafa_stock is an irregular time series of stock market data for Google, Amazon, Facebook & Apple. The stock’s volume and price information (opening, closing, adjusted closing, high and low) is available for every trading day between 2014 and 2018.\nglobal_economy contains annual macroeconomic indicators for 263 countries. The data sourced from The World Bank features GDP, Growth, CPI, Imports, Exports and Population, of which their availability varies between countries.\nhh_budget describes the characteristics of household budgets for Australia, Canada, Japan and USA. Annual indicators of debt, disposable income, expenditure, savings, wealth and unemployment are available between 1995 and 2016.\nnyc_bikes contains individual trips for 10 NYC Citi Bikes in 2018. The start and end time of the trip is available at 1 second accuracy, along with the start and end position. Some demographic information about the rider including their ride type, birth year and gender is available.\nolympic_running is a quadrennial (once every four years) dataset that contains the fastest running time for women and men’s 100m - 10000m races in the Olympics. Data from 1898 to 2016 is available, however some years are missing as the Olympics were not held during the World Wars.\nPBS provides monthly subsidised prescription counts and costs from Medicare Australia (Australia’s universal health care system). It is disaggregated by the eligibility for concession, the type of concession (after some expenditure threshold, greater subsidies are provided), and the prescription’s ATC1 and ATC2 classification.\npelt features the classic predator prey relationship between the Snowshoe Hare and the Canadian Lynx. It consists of the Hudson Bay Company’s annual trading records for pelts between 1845 and 1935.\n\n\n\nContributing to tsibbledata\nThe datasets above cover a diverse set of time series patterns. It covers regular and irregular data; non-seasonal, seasonal and multi-seasonal; frequent (30 minutes) and infrequent (4 years). Some data contain nested and crossed structures, some have relationships between variables or series.\nThe data in this package is far from comprehensively covering all types of time series. In particular, I’m searching for novel examples of time series that are unlike any other in the package. Some data types which I think will be useful additions to the package include panel/longitudinal data, higher frequency data (&lt;1 minute frequency and &lt;15 minute frequencies), and irregular seasonal patterns (such as non-gregorian calendar effects).\nIf you know of interesting datasets that meet the above criteria, or have a dataset that is unlike any other in this package, it would be great to include it in this package.\n\n\nAdditional resources\n\nThe pkgdown site\nThe GitHub repository\n\n\n\n\n\nCitationBibTeX citation:@online{o'hara-wild2019,\n  author = {O’Hara-Wild, Mitchell},\n  title = {Introducing Tsibbledata},\n  date = {2019-06-15},\n  url = {https://mitchelloharawild.com//blog/tsibbledata},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nO’Hara-Wild, Mitchell. 2019. “Introducing Tsibbledata.”\nJune 15, 2019. https://mitchelloharawild.com//blog/tsibbledata."
  },
  {
    "objectID": "blog/fable-transformations/index.html",
    "href": "blog/fable-transformations/index.html",
    "title": "Transformations in fable",
    "section": "",
    "text": "How apply different transformations across multiple time series\n\n\n\n\nCitationBibTeX citation:@online{o'hara-wild2024,\n  author = {O’Hara-Wild, Mitchell},\n  title = {Transformations in Fable},\n  date = {2024-05-25},\n  url = {https://mitchelloharawild.com//blog/fable-transformations},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nO’Hara-Wild, Mitchell. 2024. “Transformations in Fable.”\nMay 25, 2024. https://mitchelloharawild.com//blog/fable-transformations."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Transformations in fable\n\n\n\n\n\n\ntime series\n\n\nforecasting\n\n\n\nSimplifying data for modelling with transformations\n\n\n\n\n\nMay 25, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I refuse to add style options to my ggplot2 plot functions\n\n\n\n\n\n\nggplot2\n\n\npackages\n\n\n\nDesigning plot helper and grammar extension functions for ggplot2, and how good design helps you make better plots.\n\n\n\n\n\nMay 25, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing fable\n\n\n\n\n\n\nrelease\n\n\ntime series\n\n\ntidyverse\n\n\n\nForecasting models for tidy time series\n\n\n\n\n\nSep 30, 2019\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing feasts\n\n\n\n\n\n\nrelease\n\n\ntime series\n\n\ntidyverse\n\n\n\nFeature extraction and statistics for time series\n\n\n\n\n\nAug 30, 2019\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nvitae 0.2.0\n\n\n\n\n\n\nrelease\n\n\nropensci\n\n\n\nNew templates and more features\n\n\n\n\n\nJul 12, 2019\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing tsibbledata\n\n\n\n\n\n\nrelease\n\n\ndata\n\n\ntime series\n\n\ntidyverse\n\n\n\nDiverse datasets for tsibble\n\n\n\n\n\nJun 15, 2019\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing vitae\n\n\n\n\n\n\nrelease\n\n\nropensci\n\n\n\nAutomate your CV with vitae\n\n\n\n\n\nJan 9, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing taipan\n\n\n\n\n\n\nrelease\n\n\nimage\n\n\nmachine learning\n\n\nshiny\n\n\n\nAnnotate images for analysis with taipan\n\n\n\n\n\nSep 27, 2018\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nuseR! 2018 feature wall\n\n\n\n\n\n\nmagick\n\n\nuseR!2018\n\n\n\nCreating the hexmap feature wall for useR! 2018\n\n\n\n\n\nJul 11, 2018\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nArranging hex stickers in R\n\n\n\n\n\n\nmagick\n\n\npurrr\n\n\n\nFeatures magick to create the purrr-fect hex layout\n\n\n\n\n\nJul 10, 2018\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nLoading R packages in a loop\n\n\n\n\n\n\nnse\n\n\n\nNon-standard evaluation with the library function\n\n\n\n\n\nJul 4, 2018\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/vitae/index.html",
    "href": "blog/vitae/index.html",
    "title": "Introducing vitae",
    "section": "",
    "text": "r htmltools::img(src = \"vitae.png\", class = \"hex\") For those looking to refresh their CV this year, the vitae package may offer a convenient and maintainable solution. The package leverages the power of R Markdown’s dynamic documents to help you create, maintain, and even automate your Résumé or CV. The package provides a growing collection of popular CV templates, with simple data-driven functions add entries such as your education, experiences, and accolades. As the CV entries can be generated from data, it integrates well with existing and future R packages. This allows you to programatically filter your CV experiences to be more relevant for each job, and automatically download your qualifications or work experience from the web.\nThe project began at the 2018 rOpenSci OzUnconf, where R users of all levels of expertise came together to build thirteen packages and one guide. Many of the participants had never developed an R package before, and had no experience with collaborative programming with git. Those interested in this great event should read Nicholas Tierney’s (@nj_tierney) recap on the rOpenSci blog: Continuing to Grow Community Together at ozunconf, 2018.\nThe resulting package is the result of many ideas and discussions from participants of the unconference, and was primarily implemented by myself (@mitchoharawild) and Rob Hyndman (@robjhyndman). An early realisation was that we each had slightly different pronunciations for vitae, and to our astonishment we were all wrong! It’s supposed to be pronounced /ˈviːteɪ/, as in vee-tie, because it is a Latin expression - who knew?\nAs of today, the vitae package is now available on CRAN. Is it time to update your CV? Get started with the package by reading the introduction to vitae. If you’re applying for a job requiring R, and your CV is written using R, that’s got to be worth something! This blog is continued over at the rOpenSci blog in vitae: Dynamic CVs with R Markdown. In this blog I go into further detail about the design decisions, and provide some examples of how it can be used to quickly create your next CV.\n\nRead more about vitae\n\nThe pkgdown website\nThe vignettes: Introduction to vitae and Creating vitae templates\nThe GitHub repository\n\n\n\n\n\nCitationBibTeX citation:@online{o'hara-wild2019,\n  author = {O’Hara-Wild, Mitchell},\n  title = {Introducing Vitae},\n  date = {2019-01-09},\n  url = {https://mitchelloharawild.com//blog/vitae},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nO’Hara-Wild, Mitchell. 2019. “Introducing Vitae.” January\n9, 2019. https://mitchelloharawild.com//blog/vitae."
  },
  {
    "objectID": "blog/user-2018-feature-wall/index.html",
    "href": "blog/user-2018-feature-wall/index.html",
    "title": "useR! 2018 feature wall",
    "section": "",
    "text": "The hexwall at useR! 2018 features roughly 200 contributed R package hexagon stickers. If you’ve ever had difficulty aligning hexagon stickers in your slides or even on your laptop, you may appreciate the challenge of arranging hundreds of hexagons. Fortunately with R and a little bit of magick, we can substantially simplify this process."
  },
  {
    "objectID": "blog/user-2018-feature-wall/index.html#collection",
    "href": "blog/user-2018-feature-wall/index.html#collection",
    "title": "useR! 2018 feature wall",
    "section": "Collection",
    "text": "Collection\nHexagon collection proved to be one of the more time-consuming steps in the project. Without a Comprehensive R Hexagon Repository (CRHR), Di Cook (@visnut) and I resorted to collecting hexagons via email and promoting the project via twitter.\n\nOur collection efforts were also supported by Rstudio’s hex-stickers and Bioconductor’s BiocStickers repositories. We opted not to sift through the hexb.in library as we wanted the feature wall to feature only R package stickers.\nThe response to our project has been huge, and we are grateful for everyone who has contributed their stickers."
  },
  {
    "objectID": "blog/user-2018-feature-wall/index.html#hexwall",
    "href": "blog/user-2018-feature-wall/index.html#hexwall",
    "title": "useR! 2018 feature wall",
    "section": "hexwall",
    "text": "hexwall\nThe hexwall script (available at mitchelloharawild/hexwall) provides the magick for cleaning, sorting, and arranging hexagons. The function does the following operations using the ROpenSci magick package:\n\nLoad the images\nMake white backgrounds transparent\nTrim images\nRemove bad images (low resolution or incorrect dimensions)\nArrange the stickers on the canvas\n\nFor more details on using magick to arrange hexagons, you can read arranging hex stickers in R"
  },
  {
    "objectID": "blog/user-2018-feature-wall/index.html#creating-the-hex-map",
    "href": "blog/user-2018-feature-wall/index.html#creating-the-hex-map",
    "title": "useR! 2018 feature wall",
    "section": "Creating the hex map",
    "text": "Creating the hex map\nTo create a hexagon map of Australia, we first need to find a map. I’m using the GADM database to give a nice boundary of Australia, which is easily obtainable via the raster package.\n\nlibrary(tidyverse)\nlibrary(raster)\nlibrary(sf)\naus &lt;- getData(\"GADM\", country = \"AUS\", level = 0) %&gt;%\n  disaggregate() %&gt;%\n  geometry()\n\n#&gt; Warning in getData(\"GADM\", country = \"AUS\", level = 0): getData will be removed in a future version of raster\n#&gt; . Please use the geodata package instead\n\n\n#&gt; Warning in explodePolygons(x, ...): No rgeos support in sp from October 2023;\n#&gt; see https://r-spatial.org/r/2023/05/15/evolution4.html\n\nggplot() + \n  geom_sf(data = st_as_sf(aus))\n\n\n\n\n\n\n\n\nTo convert this map into hexagonal coordinates, we can use the spsample function to sample a hexagonal lattice. Through experimentation, a cellsize of 2 is roughly appropriate for the number of hexagons submitted to us. As it is a random process, some repetition may be needed to get the exact number of hexagons on the map with a nice layout.\n\nhex_points &lt;- aus %&gt;%\n  spsample(type = \"hexagonal\", cellsize = 2)\n\nas_tibble(hex_points@coords)\n\n#&gt; # A tibble: 201 x 2\n#&gt;        x     y\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  146. -43.3\n#&gt;  2  145. -41.6\n#&gt;  3  147. -41.6\n#&gt;  4  141. -38.1\n#&gt;  5  143. -38.1\n#&gt;  6  145. -38.1\n#&gt;  7  147. -38.1\n#&gt;  8  140. -36.4\n#&gt;  9  142. -36.4\n#&gt; 10  144. -36.4\n#&gt; # i 191 more rows\n\n\nTo ensure that we are happy with the hexagon placements, we should view the hexagon grid on a map. We can convert our coordinates to hexagonal polygons using HexPoints2SpatialPolygons, and then plot them on the map.\n\naus_hex &lt;- HexPoints2SpatialPolygons(hex_points, dx = 2)\n\nggplot() + \n  geom_sf(data = st_as_sf(aus)) + \n  geom_sf(data = st_as_sf(aus_hex), colour = \"blue\", fill = NA)\n\n\n\n\n\n\n\n\nLooks great, let’s make the map. Using the script from mitchelloharawild/hexwall, we provide:\n\nA folder containing the hexagon images\nDesired pixel width of each hexagon\nHexagon coordinates computed above\n\nIf there are lot of stickers to position, or the resulting image dimension is large, this may take some time.\n\nsource(\"hexwall.R\")\nhexwall(\n  \"hexstickers\",\n  sticker_width = 500,\n  coords = hex_points@coords,\n  sort_mode = \"colour\"\n)\n\n\nBeyond that, there were just a few finishing touches needed to add the useR! 2018 logo and text. Hope you all enjoy the 2018 useR! conference :)"
  },
  {
    "objectID": "blog/feasts/index.html",
    "href": "blog/feasts/index.html",
    "title": "Introducing feasts",
    "section": "",
    "text": "Feast your eyes on the latest CRAN release to the collection of tidy time series R packages. The feasts package is feature-packed with functions for understanding the behaviour of time series through visualisation, decomposition and feature extraction. The package name feasts is an acronym summarising its key features: Feature Extraction And Statistics for Time Series. Much like Earo Wang’s tsibble package, the feasts package is designed to work with multiple series observed at any time interval.\nIf you’ve used graphics from Rob Hyndman’s forecast package or features from tsfeatures, this package allows these features to be used seamlessly with tsibble and the tidyverse.\nWith the package now available on CRAN, it is now easier than ever to install:\ninstall.packages(\"feasts\")\nTo see what is on feasts’ menu, we’ll start by loading a few packages:\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(feasts)\nIn particular, we’ll be exploring the total quarterly Australian domestic overnight trips for a variety of locations and purposes around Australia.\ntourism\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# i 24,310 more rows\nThere’s plenty more datasets to explore in the tsibbledata package.\nNot sure what a tsibble is? Learn about tidy time series data in Reintroducing tsibble: data tools to melt the clock."
  },
  {
    "objectID": "blog/feasts/index.html#graphics",
    "href": "blog/feasts/index.html#graphics",
    "title": "Introducing feasts",
    "section": "Graphics",
    "text": "Graphics\nA time series can be plotted in many ways, each of which providing a different perspective into the features and structural patterns of the time series.\nThe most common graphic is a time series plot, which simply shows the data (y-axis) against time (x-axis).\n\ntourism %&gt;% \n  group_by(Purpose) %&gt;% \n  summarise(Trips = sum(Trips)) %&gt;% \n  autoplot(Trips)\n\n\n\n\n\n\n\n\nThis allows us to see the big picture, and especially evident is that Holiday travel is the most common domestic tourism purpose. We can also see a substantial increase in trips after 2010. Holiday and business trips show the most seasonality, but it is unclear which quarter has the most trips.\nFor a closer look at seasonality we can see seasonal plots (gg_season()) and seasonal subseries plots (gg_subseries()).\n\ntourism %&gt;% \n  group_by(Purpose) %&gt;% \n  summarise(Trips = sum(Trips)) %&gt;% \n  gg_season(Trips)\n\n\n\n\n\n\n\n\nA seasonal plot wraps the time axis by each seasonal period (in this case years), to more clearly display the seasonality in the data. It is clear from the Holiday facet that most trips are made in Q1, possibly to make the most of Australia’s hot summer days. It also seems that Q1 (summer) and Q4 (spring) are popular times to visit friends and family, with Q2 (autumn) and Q3 (winter) being a common time for business trips.\n\ntourism %&gt;% \n  group_by(Purpose) %&gt;% \n  summarise(Trips = sum(Trips)) %&gt;% \n  gg_subseries(Trips)\n\n\n\n\n\n\n\n\nThe subseries plot is especially useful for identifying changes in seasonal patterns over time. It produces a set of data subseries for the times within each seasonal period. The above shows separate plots consisting of the data from the same quarter, and the blue line indicates the average. From this plot we can see that Q4 business trips have stopped growing, whilst Q1-Q3 continue to trend upward.\nThe feasts package can also do many more time series graphics, including:\n\nLag plots gg_lag()\nAutocorrelation plots (autoplot() of ACF(), PACF(), or CCF())\nEnsemble plots for time series (gg_tsdisplay())\nEnsemble plots for time series residuals (gg_tsresiduals())\nPlots of characteristic ARMA root (gg_arma())\n\nTo see more of these plots in action, check out Rob Hyndman’s blog post on Time series graphics with feasts.\nAlso try out Earo Wang’s incredible calendar plots (facet_calendar() 🤯) from her sugrrants package!"
  },
  {
    "objectID": "blog/feasts/index.html#decompositions",
    "href": "blog/feasts/index.html#decompositions",
    "title": "Introducing feasts",
    "section": "Decompositions",
    "text": "Decompositions\nThe first release of feasts contains two decompositions which tidy up existing functions from the stats package.\n\n\n\n\n\n\n\n\n\n\nMethod\nstats\nfeasts\n\n\n\n\nClassical seasonal decomposition\ndecompose()\nclassical_decomposition()\n\n\nLoess seasonal decomposition\nstl()\nSTL()\n\n\n\n\n\nTime series decompositions allow you to isolate structural components such as trend and seasonality from the data. The decomposition functions in feasts use a model-like formula interface, allowing you to control many aspects of the decomposition (using season(window = 5) allows the seasonality to change fairy quickly for quarterly data).\n\ntourism %&gt;% \n  group_by(Purpose) %&gt;% \n  summarise(Trips = sum(Trips)) %&gt;% \n  model(STL(Trips ~ season(window = 5))) %&gt;% \n  components()\n\n# A dable: 320 x 8 [1Q]\n# Key:     Purpose, .model [4]\n# :        Trips = trend + season_year + remainder\n   Purpose  .model       Quarter Trips trend season_year remainder season_adjust\n   &lt;chr&gt;    &lt;chr&gt;          &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Business STL(Trips ~~ 1998 Q1 3599. 3814.     -425.       210.          4024.\n 2 Business STL(Trips ~~ 1998 Q2 3724. 3854.      185.      -315.          3539.\n 3 Business STL(Trips ~~ 1998 Q3 4356. 3895.      296.       165.          4060.\n 4 Business STL(Trips ~~ 1998 Q4 3796. 3936.      -42.9      -97.7         3838.\n 5 Business STL(Trips ~~ 1999 Q1 3335. 3991.     -431.      -225.          3766.\n 6 Business STL(Trips ~~ 1999 Q2 4714. 3986.      174.       555.          4540.\n 7 Business STL(Trips ~~ 1999 Q3 4190. 3986.      285.       -81.4         3905.\n 8 Business STL(Trips ~~ 1999 Q4 3701. 3970.        1.45    -271.          3700.\n 9 Business STL(Trips ~~ 2000 Q1 3562. 3943.     -464.        83.8         4026.\n10 Business STL(Trips ~~ 2000 Q2 4018. 3991.      162.      -135.          3856.\n# i 310 more rows\n\n\nThe decomposed table (dable) explains how the Trips variable has been split into three new series via Trips = trend + season_year + remainder, which has been done for all four travel purposes.\n\ntourism %&gt;% \n  group_by(Purpose) %&gt;% \n  summarise(Trips = sum(Trips)) %&gt;% \n  model(STL(Trips ~ season(window = 5))) %&gt;% \n  components() %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nThe dable can also be plotted to show how each of the decomposed components vary between series and time. A dable also behaves very similarly to a tsibble, allowing you to visualise the seasonality without distractions of trend and remainder terms.\n\ntourism %&gt;% \n  group_by(Purpose) %&gt;% \n  summarise(Trips = sum(Trips)) %&gt;% \n  model(STL(Trips ~ season(window = 5))) %&gt;%\n  components() %&gt;% \n  gg_season(season_year)"
  },
  {
    "objectID": "blog/feasts/index.html#features",
    "href": "blog/feasts/index.html#features",
    "title": "Introducing feasts",
    "section": "Features",
    "text": "Features\nA big feature of feasts is the ability to extract a set of features (single value summary of data characteristics) from the time series.\nFeatures are particularly useful for visualising large collections of time series. Plotting multiple time series using the above methods do not scale very well beyond 10 similar series, and the original tourism dataset contains 304 (and even more if you consider aggregates)!\nThe STL decomposition above can be used to compute features about the strength of the its trend and seasonality components, allowing us to see an overview of the entire dataset’s patterns.\n\ntourism %&gt;% \n  features(Trips, feature_set(tags = \"stl\"))\n\n# A tibble: 304 x 12\n   Region State Purpose trend_strength seasonal_strength_year seasonal_peak_year\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 Adela~ Sout~ Busine~          0.464                  0.407                  3\n 2 Adela~ Sout~ Holiday          0.554                  0.619                  1\n 3 Adela~ Sout~ Other            0.746                  0.202                  2\n 4 Adela~ Sout~ Visiti~          0.435                  0.452                  1\n 5 Adela~ Sout~ Busine~          0.464                  0.179                  3\n 6 Adela~ Sout~ Holiday          0.528                  0.296                  2\n 7 Adela~ Sout~ Other            0.593                  0.404                  2\n 8 Adela~ Sout~ Visiti~          0.488                  0.254                  0\n 9 Alice~ Nort~ Busine~          0.534                  0.251                  0\n10 Alice~ Nort~ Holiday          0.381                  0.832                  3\n# i 294 more rows\n# i 6 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;\n\n\nThe features from an STL decomposition also describe other behaviours such as how linear, curved, and spiky the data is. This dataset of features can be used with ggplot2 (and other packages) to produce stunning overviews of many time series.\n\ntourism %&gt;% \n  features(Trips, feature_set(tags = \"stl\")) %&gt;% \n  ggplot(aes(x = trend_strength, y = seasonal_strength_year, colour = Purpose)) + \n  geom_point() + \n  stat_density_2d(aes(fill = Purpose, alpha = ..level..), bins = 5, geom = \"polygon\") + \n  facet_wrap(vars(Purpose), nrow = 1) +\n  coord_equal() + \n  xlim(c(0,1)) + ylim(c(0,1)) + \n  labs(x = \"Trend strength\", y = \"Seasonal strength\") + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nHere, it is clear that holiday travel is the most seasonal purpose of tourism in Australia, however this is only the case for some regions. The strength of trend in these series varies similarly across travel purpose.\nThe first release includes a total of 42 feature metrics, which can be computed together using feature_set(pkgs = \"feasts\"). A list of all currently available features can be found in links from the ?feature_set help file."
  },
  {
    "objectID": "blog/fable/index.html",
    "href": "blog/fable/index.html",
    "title": "Introducing fable",
    "section": "",
    "text": "The fable package bridges the gap between popular tidy data analysis workflows and time series forecasting. Using tidy temporal data from Earo Wang’s tsibble package, fable allows complex forecasting tasks to be performed with ease. The package is the next iteration of Rob Hyndman’s forecast package, providing the tools and extensibility support needed to overcome current and future time series challenges. I’m ecstatic to announce that the fable package is now available on CRAN! So whether you’re a seasoned forecasting fanatic or you’re making your first foray into forecasting, getting started is easier than ever:\n\ninstall.packages(\"fable\")\n\n\nfable: forecasting with tables\nA major difference between fable and its predecessor forecast is the way in which time series data is stored. A substantial source of confusion for forecast package users is the creation and use of ts objects. The fable package is built upon the tsibble package, which (as the name suggests) is a tibble with time series structure. Among many benefits, the data is now in the same structure provided in most data files. This makes loading time series data into R a breeze (and less error-prone), as the column of data specifying the observation’s measurement time is now actually used in the data! Other benefits of a tabular data structure include integration with non-temporal packages and support for multiple time series with mixed measurement types.\n\nA fable is never true, but it tells you something important about reality - and that’s what a forecast is.\nRob Hyndman (2018-06-21)\nNew York Open Statistical Programming Meetup (https://youtu.be/yx6OQ-8HofU?t=2484)\n\n\n\n\nRob Hyndman introducing fable to the nyhackr group\n\n\nThe fable package has been actively developed over the past two years, and has undergone a couple of substantial revisions to the interface prior to release. Despite these changes the package’s goals have remained constant:\n\nSeamlessly integrate within a tidy data analysis workflow.\nProvide a consistent, intuitive and familiar interface for time series modelling.\nSimple forecasting (and reconciliation) of many related time series.\nEncourage extensibility by developing general forecasting tools.\nProvide forecasts with distributions, not intervals.\nNaturally support sub-daily and high frequency data with complex structures.\n\nThe first release of fable implements all of these goals and more. While this post won’t cover the details of how these goals are achieved, I hope that the forecasting example below illustrates how these goals have been realised for forecasting in practice. I have no doubt that there will be many more blog posts which will explore fable in greater detail.\n\n\nForecasting with fable\n\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(fable)\n\nTo see how fable can be applied to real forecasting problems we’ll create some simple forecasts of Australia’s domestic tourism. This is the same dataset analysed in the Introducing feasts post, which provides a visual introduction to the data using the closely related feasts package.\n\nData manipulation and exploration\n\ntourism\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# i 24,310 more rows\n\n\nThis dataset contains quarterly domestic overnight trips for a variety of locations and purposes around Australia. When disaggregated by the key variables (Region, State and Purpose), we have a total of 304 separate time series to forecast.\nLet’s start simple and use dplyr to calculate the total overnight domestic trips for Australia.\n\ntourism_aus &lt;- tourism %&gt;% \n  summarise(Trips = sum(Trips))\ntourism_aus\n\n# A tsibble: 80 x 2 [1Q]\n   Quarter  Trips\n     &lt;qtr&gt;  &lt;dbl&gt;\n 1 1998 Q1 23182.\n 2 1998 Q2 20323.\n 3 1998 Q3 19827.\n 4 1998 Q4 20830.\n 5 1999 Q1 22087.\n 6 1999 Q2 21458.\n 7 1999 Q3 19914.\n 8 1999 Q4 20028.\n 9 2000 Q1 22339.\n10 2000 Q2 19941.\n# i 70 more rows\n\n\nAt minimum, we should plot the data before considering a model for it. A tsibble dataset works seamlessly with ggplot2, allowing you to design informative graphics for this data. For a quick look at the data we also support autoplot() functionality (and more time series plots discussed in Introducing feasts).\n\ntourism_aus %&gt;% \n  autoplot(Trips)\n\n\n\n\n\n\n\n\nThe first step to forecasting this data would be to identify appropriate model(s). A seasonal model would be required as the data shows signs of seasonality. Including trend would also be helpful, although as the trend has changed over time (becoming positive after 2010) our model will need to support this too. Considering this, an exponential smoothing model may be suitable for this data.\n\n\nModel specification\nModel specification in fable supports a formula based interface (much like lm() and other cross-sectional modelling functions). A model formula in R is expressed using response ~ terms, where the formula’s left side describes the response (and any transformations), while the right describes terms used to model the response. The terms of a fable model often include model specific functions called ‘specials’. They describe how the time series dynamics are captured by the model, and the supported specials can be found in the method’s help file.\nExponential smoothing models are defined using the ETS() function, which provides ‘specials’ for controlling the error(), trend() and season(). These time series elements appear to be additively combined to give the response, and so an appropriate model specification may be:\n\nETS(Trips ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n\nIdentifying an appropriate model specification can be tricky as it requires some background knowledge about temporal patterns and ETS models. Don’t be discouraged! If your unsure, you can let ETS() and other models automatically choose the best specification if multiple options are provided. So if you can’t tell if the seasonality is additive (season(\"A\")) or multiplicative (season(\"M\")), you can let fable decide via:\n\nETS(Trips ~ error(\"A\") + trend(\"A\") + season(c(\"A\", \"M\")))\n\nIn fact this automatic selection is the default option. If the season() special is not specified (excluded entirely from formula), the seasonal structure will be automatically chosen as either none, additive or multiplicative seasonality with season(c(\"N\", \"A\", \"M\")). Automatic selection also occurs when error() and trend() are not specified, allowing an appropriate ETS model to be determined fully automatically with:\n\nETS(Trips)\n\n\n\nModel estimation\nA model is estimated using the model() function, which uses a dataset to train one or more specified models.\n\nfit &lt;- tourism_aus %&gt;% \n  model(auto_ets = ETS(Trips))\nfit\n\n# A mable: 1 x 1\n      auto_ets\n       &lt;model&gt;\n1 &lt;ETS(A,A,A)&gt;\n\n\nThe resulting mable (model table) object informs us that an ETS(A,A,A) model has been automatically selected. Within that cell a complete description of the model is stored, including everything needed to produce forecasts (such as estimated coefficients). The report() function can be used if the mable contains only one model, which provides a familiar display of the models estimates and summary measures.\n\nreport(fit)\n\nSeries: Trips \nModel: ETS(A,A,A) \n  Smoothing parameters:\n    alpha = 0.4495675 \n    beta  = 0.04450178 \n    gamma = 0.0001000075 \n\n  Initial states:\n     l[0]      b[0]      s[0]     s[-1]     s[-2]    s[-3]\n 21689.64 -58.46946 -125.8548 -816.3416 -324.5553 1266.752\n\n  sigma^2:  699901.4\n\n     AIC     AICc      BIC \n1436.829 1439.400 1458.267 \n\n\nThe package also supports verbs from the broom package, allowing you to tidy() your coefficients, glance() your model summary statistics, and augment() your data with predictions. These verbs provide convenient and consistent methods for accessing useful values from an estimated model.\n\n\nProducing forecasts\nThe forecast() function is used to produce forecasts from estimated models. The forecast horizon (h) is used to specify how far into the future forecasts should be made. h can be specified with a number (the number of future observations) or text (the length of time to predict). You can also specify the time periods to predict using new_data, which allows you to provide a tsibble of future time points to forecast, along with any exogenous regressors which may be required by the model.\n\nfc &lt;- fit %&gt;% \n  forecast(h = \"2 years\")\nfc\n\n# A fable: 8 x 4 [1Q]\n# Key:     .model [1]\n  .model   Quarter             Trips  .mean\n  &lt;chr&gt;      &lt;qtr&gt;            &lt;dist&gt;  &lt;dbl&gt;\n1 auto_ets 2018 Q1   N(29068, 7e+05) 29068.\n2 auto_ets 2018 Q2  N(27794, 870750) 27794.\n3 auto_ets 2018 Q3 N(27619, 1073763) 27619.\n4 auto_ets 2018 Q4 N(28627, 1311711) 28627.\n5 auto_ets 2019 Q1 N(30336, 1587455) 30336.\n6 auto_ets 2019 Q2 N(29062, 1903591) 29062.\n7 auto_ets 2019 Q3 N(28887, 2262980) 28887.\n8 auto_ets 2019 Q4 N(29895, 2668392) 29895.\n\n\nYou’ll notice that this function gives us a fable (forecast table), which contains point forecasts in the Trips column, and the forecast’s distribution in the .distribution column. If we had specified a transformation in the model specification (say ETS(log(Trips))), the resulting forecasts would be automatically back transformed and adjusted for bias.\nWhile using and storing distributions is powerful, they can be more difficult to interpret than intervals. Forecast intervals can be extracted from a forecast distribution using the hilo() function:\n\nfc %&gt;% \n  hilo(level = c(80, 95))\n\n# A tsibble: 8 x 6 [1Q]\n# Key:       .model [1]\n  .model   Quarter             Trips  .mean                  `80%`\n  &lt;chr&gt;      &lt;qtr&gt;            &lt;dist&gt;  &lt;dbl&gt;                 &lt;hilo&gt;\n1 auto_ets 2018 Q1   N(29068, 7e+05) 29068. [27995.95, 30140.25]80\n2 auto_ets 2018 Q2  N(27794, 870750) 27794. [26597.85, 28989.59]80\n3 auto_ets 2018 Q3 N(27619, 1073763) 27619. [26291.05, 28947.01]80\n4 auto_ets 2018 Q4 N(28627, 1311711) 28627. [27158.76, 30094.28]80\n5 auto_ets 2019 Q1 N(30336, 1587455) 30336. [28721.43, 31950.79]80\n6 auto_ets 2019 Q2 N(29062, 1903591) 29062. [27293.57, 30829.90]80\n7 auto_ets 2019 Q3 N(28887, 2262980) 28887. [26959.18, 30814.90]80\n8 auto_ets 2019 Q4 N(29895, 2668392) 29895. [27801.09, 31987.98]80\n# i 1 more variable: `95%` &lt;hilo&gt;\n\n\nRather than reading values from a table, it is usually easier to evaluate forecast behaviour by making a plot. Much like plotting a tsibble, we have provided autoplot() and autolayer() methods for plotting forecasts. Unlike the forecast package, fable does not store the original data and fitted model in the fable object, so the historical data must be passed in to see it on the plot.\n\nfc %&gt;% \n  autoplot(tourism_aus)\n\n\n\n\n\n\n\n\n\n\nChoosing the best model\nWhile ETS() has been able to choose the best ETS model for this data, a different model class may give even better results. The model() function is capable of estimating many specified models. Let’s compare the ETS model with an automatically selected ARIMA() model (much like forecast::auto.arima()) and a linear model (TSLM()) with linear time trend and dummy seasonality.\n\nfit &lt;- tourism_aus %&gt;% \n  model(\n    ets = ETS(Trips),\n    arima = ARIMA(Trips),\n    lm = TSLM(Trips ~ trend() + season())\n  )\nfit\n\n# A mable: 1 x 3\n           ets                    arima      lm\n       &lt;model&gt;                  &lt;model&gt; &lt;model&gt;\n1 &lt;ETS(A,A,A)&gt; &lt;ARIMA(0,1,1)(0,1,1)[4]&gt;  &lt;TSLM&gt;\n\n\nThe mable now contains three models, each specified model is stored in a separate column.\nWe can produce forecasts and visualise the results using the same code as before. To minimise overplotting I have chosen to only show the 80% forecast interval, and have made the forecasts semi-transparent.\n\nfit %&gt;% \n  forecast(h = \"2 years\") %&gt;% \n  autoplot(tourism_aus, level = 80, alpha = 0.5)\n\n\n\n\n\n\n\n\nIt is clear from this plot that the linear model (lm) is unable to capture the trend change at 2010. The linear model could be improved by using a piecewise linear trend with a knot at 2010, but I’ll leave that for you to try (replace trend() with trend(knots = yearquarter(\"2010 Q1\"))).\nVisually distinguishing the best model between ETS and ARIMA is difficult. The ETS model predicts a stronger trend than the ARIMA model, and both produce very similar seasonal patterns.\nTo choose the best model we can make use of numerical accuracy measures using the accuracy() function. This function can compute various accuracy measures based on point forecasts, forecast intervals and forecast distributions. It also allows you to specify your own accuracy measure functions.\nTraining (in-sample) accuracy will be given when applied to a mable.\n\naccuracy(fit)\n\n# A tibble: 3 x 10\n  .model .type           ME  RMSE   MAE    MPE  MAPE  MASE RMSSE     ACF1\n  &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ets    Training  1.05e+ 2  794.  604.  0.379  2.86 0.636 0.653 -0.00151\n2 arima  Training  1.54e+ 2  840.  632.  0.584  2.97 0.666 0.691 -0.0432 \n3 lm     Training -1.82e-13 1715. 1436. -0.597  6.67 1.51  1.41   0.816  \n\n\nThe in-sample accuracy suggests that the ETS model performs best. This is because it has the lowest values for all accuracy measures (lower values indicate less errors). As expected, the linear model is much worse than the others.\nForecast (out-of-sample) accuracy will be computed when a fable is used with accuracy(). Note that you will need to withhold a test set to base your accuracy on.\n\ntourism_aus %&gt;% \n  # Withhold the last 3 years before fitting the model\n  filter(Quarter &lt; yearquarter(\"2015 Q1\")) %&gt;% \n  # Estimate the models on the training data (1998-2014)\n  model(\n    ets = ETS(Trips),\n    arima = ARIMA(Trips),\n    lm = TSLM(Trips ~ trend() + season())\n  ) %&gt;% \n  # Forecast the witheld time peroid (2015-2017)\n  forecast(h = \"3 years\") %&gt;% \n  # Compute accuracy of the forecasts relative to the actual data \n  accuracy(tourism_aus)\n\n# A tibble: 3 x 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arima  Test  1397. 1795. 1452.  5.31  5.54  1.58  1.49 0.496\n2 ets    Test  1895. 2281. 1909.  7.23  7.28  2.08  1.90 0.613\n3 lm     Test  4664. 4822. 4664. 18.1  18.1   5.09  4.01 0.634\n\n\nThe out-of-sample accuracy shows that the ARIMA model produced the most accurate forecasts for 2015-2017 using data from 1998-2014.\nSo which model is best? In-sample (training) accuracy uses one-step ahead forecast errors from model coefficients based on the whole data. In many senses this is unrealistic, as the forecasts are partially based on information not available when forecasting into the future. Additionally, these forecasts are based only on one-step ahead accuracy, where in practice you may be interested in forecasting a few years ahead.\nAlternatively, out-of-sample (test) accuracy is more akin to the actual forecasting task: predicting the future using only past information. The accuracy is based on forecast errors from three years of data never seen by the model. This advantage is also a problem, as the accuracy is now based on just 12 values, so the performance is more sensitive to chance. Calculating accuracy using time series cross-validation overcomes many of these problems, but will take more time to compute.\nIn short, both ETS and ARIMA models are producing reasonable forecasts for this data. Instead of choosing a favourite, we can do better by averaging them which usually gives better results.\n\nfit &lt;- tourism_aus %&gt;% \n  model(\n    ets = ETS(Trips),\n    arima = ARIMA(Trips)\n  ) %&gt;% \n  mutate(\n    average = (ets + arima) / 2\n  )\nfit\n\n# A mable: 1 x 3\n           ets                    arima       average\n       &lt;model&gt;                  &lt;model&gt;       &lt;model&gt;\n1 &lt;ETS(A,A,A)&gt; &lt;ARIMA(0,1,1)(0,1,1)[4]&gt; &lt;COMBINATION&gt;\n\n\n\nfit %&gt;% \n  forecast(h = \"2 years\") %&gt;% \n  autoplot(tourism_aus, level = 80, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nScaling it up\nProducing forecasts for a single time series isn’t particularly exciting, and certainly doesn’t align well with modern forecasting problems. Suppose we were interested in forecasting tourism for each of Australia’s major states (and territories).\n\ntourism_state &lt;- tourism %&gt;% \n  group_by(State) %&gt;% \n  summarise(Trips = sum(Trips))\ntourism_state\n\n# A tsibble: 640 x 3 [1Q]\n# Key:       State [8]\n   State Quarter Trips\n   &lt;chr&gt;   &lt;qtr&gt; &lt;dbl&gt;\n 1 ACT   1998 Q1  551.\n 2 ACT   1998 Q2  416.\n 3 ACT   1998 Q3  436.\n 4 ACT   1998 Q4  450.\n 5 ACT   1999 Q1  379.\n 6 ACT   1999 Q2  558.\n 7 ACT   1999 Q3  449.\n 8 ACT   1999 Q4  595.\n 9 ACT   2000 Q1  600.\n10 ACT   2000 Q2  557.\n# i 630 more rows\n\n\nThe data now contains 8 separate time series, each with different time series characteristics:\n\ntourism_state %&gt;% \n  autoplot(Trips)\n\n\n\n\n\n\n\n\nThis is where the automatic model selection in fable is particularly useful. The model() function will estimate a specified model to all series in the data, so producing many models is simple.\n\nfit &lt;- tourism_state %&gt;% \n  model(\n    ets = ETS(Trips),\n    arima = ARIMA(Trips)\n  ) %&gt;% \n  mutate(\n    average = (ets + arima)/2\n  )\nfit\n\n# A mable: 8 x 4\n# Key:     State [8]\n  State                       ets                    arima       average\n  &lt;chr&gt;                   &lt;model&gt;                  &lt;model&gt;       &lt;model&gt;\n1 ACT                &lt;ETS(M,A,N)&gt;           &lt;ARIMA(0,1,1)&gt; &lt;COMBINATION&gt;\n2 New South Wales    &lt;ETS(A,N,A)&gt; &lt;ARIMA(0,1,1)(0,1,1)[4]&gt; &lt;COMBINATION&gt;\n3 Northern Territory &lt;ETS(M,N,M)&gt; &lt;ARIMA(1,0,1)(0,1,1)[4]&gt; &lt;COMBINATION&gt;\n4 Queensland         &lt;ETS(A,N,A)&gt;           &lt;ARIMA(2,1,2)&gt; &lt;COMBINATION&gt;\n5 South Australia    &lt;ETS(M,N,A)&gt; &lt;ARIMA(1,0,1)(0,1,1)[4]&gt; &lt;COMBINATION&gt;\n6 Tasmania           &lt;ETS(M,N,M)&gt; &lt;ARIMA(0,0,3)(2,1,0)[4]&gt; &lt;COMBINATION&gt;\n7 Victoria           &lt;ETS(M,N,M)&gt; &lt;ARIMA(0,1,1)(0,1,1)[4]&gt; &lt;COMBINATION&gt;\n8 Western Australia  &lt;ETS(M,N,M)&gt;           &lt;ARIMA(0,1,3)&gt; &lt;COMBINATION&gt;\n\n\nEach row of a mable corresponds to a separate time series (uniquely identified by its keys). From the output we can see a wide variety of models have been chosen. Some models have trend, others have seasonality, some have neither trend nor seasonality!\nProducing forecasts and evaluating accuracy is no different whether you’re modelling one time series or a hundred.\n\nfit %&gt;% \n  forecast(h = \"2 years\") %&gt;% \n  autoplot(tourism_state, level = NULL)\n\n\n\n\n\n\n\n\n\n\n\nExtensibility with fabletools\nExtensibility is at the core of fable. fable is actually built as an extension of the more general fabletools package. This allows extension modelling package developers to focus on writing methods specific to their model. More general methods used in modelling and forecasting are provided automatically by fabletools.\nDepending on the chosen model, the following features are supported:\n\nForecasting (forecast())\nMissing value interpolation (interpolate())\nReporting model output (report())\nSimulation of future paths (generate())\nStreaming new data (stream())\nRe-estimation (refit())\nDecomposition of model components (components())\nModel equation output (equation())\nBroom verbs (augment(), coef()/tidy(), glance())\nModel fits (fitted(), residuals())\n\nGeneral fabletools functionality for all models include:\n\nSeamless integration with existing packages in the tidyverse\nTools for consistent modelling interface design (such as formula parsing)\nResponse transformations and back-transformations with bias adjustment\nBatch modelling with parallelisation\nForecast distributions and intervals\nVisualisation of time series, decompositions, and forecasts (autoplot())\nAccuracy evaluation (accuracy()) with many accuracy measures\nModel combination (combination_model() / (ets + arima)/2)\nDecomposition modelling (decomposition_model())\nForecast reconciliation (reconcile(), min_trace())\n\nKeep an eye out for more extension modelling packages being released this year!\n\n\nRead more about fable\nWhile this blog post is long and covers a lot of things about forecasting with fable, it is far from comprehensive. Currently, the best resource for learning forecasting with fable is the Forecasting: Principles and Practices (3ed.) book, which is freely available online.\nSome other places with more information about fable include:\n\nThe pkgdown website\nRob Hyndman’s blog post: Tidy forecasting in R\nThe Introduction to fable vignette\nUseR!2019 talk: Flexible futures for fable functionality\nThe GitHub repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{o'hara-wild2019,\n  author = {O’Hara-Wild, Mitchell},\n  title = {Introducing Fable},\n  date = {2019-09-30},\n  url = {https://mitchelloharawild.com//blog/fable},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nO’Hara-Wild, Mitchell. 2019. “Introducing Fable.” September\n30, 2019. https://mitchelloharawild.com//blog/fable."
  },
  {
    "objectID": "blog/ggplot2-packages/index.html",
    "href": "blog/ggplot2-packages/index.html",
    "title": "Why I refuse to add style options to my ggplot2 plot functions",
    "section": "",
    "text": "I develop many packages with functions that use ggplot2 to make a specific plot, and I’m often asked if I could add new arguments for changing the style, appearance, or layout of the plot. My answer is almost always1 a resounding no!, followed by encouragement to make the plot themselves without the plot helper function.\nThis outright refusal to implement easy plot improvements might surprise users and seem harsh - after all it is easy to add an argument for changing plot colours thanks to ggplot2’s grammar-based design.\nWhy do I refuse to make these easy improvements? It’s a matter of design."
  },
  {
    "objectID": "blog/ggplot2-packages/index.html#design-patterns-of-ggplot2-extensions",
    "href": "blog/ggplot2-packages/index.html#design-patterns-of-ggplot2-extensions",
    "title": "Why I refuse to add style options to my ggplot2 plot functions",
    "section": "Design patterns of ggplot2 extensions",
    "text": "Design patterns of ggplot2 extensions\nExtension2 functions of ggplot2 can be categorised into two distinct groups:\n\nFunctions which use ggplot2 (plot helpers)\nThese functions produce a specific plot with a single function, useful for quickly creating a common plot for the data or analysis.\n\n\n\n\n\n\nExample: autoplot() for time-series plots\n\n\n\n\n\n\nlibrary(fable)\ntsibble::as_tsibble(USAccDeaths) |&gt; \n  autoplot(value)\n\n\n\n\n\n\n\n\n\n\n\nFunctions which extend ggplot2 (grammar extensions)\nThese functions add to the plotting grammar available in ggplot2, and are highly customisable and versatile. On their own, grammar extension functions don’t produce plots but rather plot elements - plots are constructed using a combination of plot elements.\n\n\n\n\n\n\nExample: ggrepel::geom_text_repel()\n\n\n\n\n\n\nlibrary(ggrepel)\nggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) +\n  geom_text_repel() +\n  geom_point(color = 'red')\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot helpers and ggplot2 extension functions are both useful, but serve different purposes in an analysis . The design tendency of doing too much can make these functions less useful for their purpose. In particular for ggplot2 extension packages, it is common to make plot helpers more extensible and ggplot2 extensions more helpful.\nWhy does this become a design problem? Read on…\n\n\n\n\n\n\n\n\nPlot helpers using ggplot2\nPlot helper functions use many ggplot2 elements together into a simple function for creating a specific graphic. These functions abstract away the grammar of graphics, and their design should focus on what is being plotted (the data and its patterns) rather than how (the geometries, aesthetic mappings, and more).\nTake for example the feasts::gg_season() function for plotting a seasonal plot of time-series data, which is commonly used in a time-series analysis to understand the shape of the seasonality (identifying peak times from throughs).\n\nlibrary(tsibble)\nas_tsibble(USAccDeaths) |&gt; \n  feasts::gg_season(y = value)\n\n\n\n\n\n\n\n\nFunctions like these are really useful during an analysis, allowing a quick peek at the data without needing to repeatedly write the same grammar elements (expand below) each and every time.\n\n\n\n\n\n\nReproduction of above plot without gg_season()\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(lubridate)\ntsibble::as_tsibble(USAccDeaths) |&gt; \n  ggplot(aes(x = month(index), y = value, colour = factor(year(index)))) + \n  geom_path() + \n  scale_x_continuous(breaks = 1:12, labels = month.abb) + \n  labs(y = \"Accidental deaths in the USA\", x = \"Month\", colour = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe handling of the x-axis wrapping and line grouping/colouring is combined/abstracted as the seasonal period. Similar plot-specific abstraction is done for faceting seasons with facet_period allowing for quick and informative plots.\n\nfill_gaps(pedestrian) |&gt; \n  dplyr::filter(Sensor == \"Southern Cross Station\") |&gt; \n  feasts::gg_season(y = Count, period = \"day\", facet_period = \"1 weeks\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproduction of above plot without gg_season()\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(lubridate)\nfill_gaps(pedestrian) |&gt; \n  dplyr::filter(Sensor == \"Southern Cross Station\") |&gt; \n  ggplot(aes(x = hour(Date_Time), y = Count, colour = as.Date(Date_Time), group = as.Date(Date_Time))) + \n  geom_path() + \n  facet_grid(cols = vars(wday(Date_Time, label = TRUE, abbr = FALSE, week_start = 1))) + \n  scale_color_gradientn(colours = scales::hue_pal()(9), labels = \\(x) as.Date(x, origin = \"1970-01-01\")) +\n  labs(y = \"Pedestrians at Southern Cross Station\", x = \"Hour\", colour = \"Date\")\n\n\n\n\n\n\n\n\n\n\n\nWhile useful, these plots aren’t the prettiest - they don’t try to be. Seasonal plots as shown above are unlikely to be readily suitable for wider publication, and the necessary improvements aren’t limited to the theme and style. A useful similar alternative is using box-plots rather than lines to more clearly show the typical number of pedestrians throughout the day. Creating beautiful publication ready graphics involves more than simply using a suitable chart type and variables, but also carefully considering the layout, colours, and typography3.\nBy not adding stylistic options to my plot helper functions, I hope to encourage users to think more creatively when constructing their visualisation. Not just using prettier colours or themes, but also creating more suitable chart types for better reader comprehension. After all, these plots are intended for experienced analysts to quickly preview important features of the data. They are aren’t intended to be readily understood by an untrained reader.\nCynthia Huang’s companion blog post Layer arguments for ggplot2 wrapper functions highlights the difficulty in creating plot helper functions which also expose the underlying grammar for further plot customisation. This design challenge is not specific to ggplot2 extensions, but more broadly applicable to any functions which abstract complexity away from underlying functions. The post offers a clever alternative that allows full plot customisation without adding countless style arguments which re-introduce complexity to an abstraction.\n\n\n\n\n\n\n\n\nGrammar extensions for ggplot2\nContrary to plot helpers, grammar extensions are all about how something is plotted using composable visualisation elements. These extension functions directly add to the ggplot2 grammar with new geometries, statistics, coordinates, scales, themes and/or more. The key advantage of directly extending the grammar is that the added functionality can be used to create any number of plots in combination with other grammar elements (including from other extension packages).\n\nGrammar extensions can be more difficult to implement than plot helper functions, since they involve understanding the inner workings or ggplot24 rather than simply how ggplot2 is used. Designing grammar extensions also require an understanding of how various plot elements work together. Well designed grammar extension functions are highly modular, making them compatible with other plot elements from ggplot2 and extension packages. This composable grammar-based design is what allows ggplot2 to create any number of plots by reusing grammar elements in different ways.\n\n\n\nGrammar extensions often have the opposite design problems as plot helper functions. In attempts to be more helpful and easy to use, grammar extensions compound multiple elements of the grammar. This makes the extensions less modular and reusable in creating other plots. This is commonly seen in the design of geometries (geom_*()) and statistics (stat_*()), which are coupled together in ggplot2 as layers. An example of this is geom_histogram(), which is actually a geom_bar()5 with a stat_bin() statistic6. This helpful histogram ‘geometry’ is less reusable as a modular component in other plots, and potentially adds confusion between similar geometries. Creating histograms as binned bar charts invites thinking about other geometries which could show binned counts, or how other statistics can be used to show proportions instead of counts."
  },
  {
    "objectID": "blog/ggplot2-packages/index.html#design-story-of-forecast-visualisation",
    "href": "blog/ggplot2-packages/index.html#design-story-of-forecast-visualisation",
    "title": "Why I refuse to add style options to my ggplot2 plot functions",
    "section": "Design story of forecast visualisation",
    "text": "Design story of forecast visualisation\n\n\n\n\nIn my package design experience, I find that the best plot helpers are simple/helpful for creating a specific plot, and ggplot2 extensions are modular/flexible as a single element in a wider grammar.\n\n\nHelpful plots for forecast visualisation\nWhen creating new plotting functions I often start with creating plot helpers, using existing ggplot2 elements and data pre-processing to construct a new plot which is somewhat complicated to create directly with ggplot2. An example of this is when I created an autoplot() function7 for the forecast package’s forecast plots.\n\nlibrary(forecast)\nwine_fc &lt;- hw(wineind, h=48)\nautoplot(wine_fc)\n\n\n\n\n\n\n\n\n\nThis autoplot() method originally re-organised a forecast object into a data frame for plotting, and used a carefully constructed ribbon geometry (geom_ribbon()) to layer the forecast intervals atop one another (along with geom_line() for the point forecast).\n\n\nNew plot elements for forecast uncertainty\nThis plot helper function met the needs for visualising forecasts for a while, but eventually came the need for visualising forecasts in different ways with different styles. Rather than adding additional arguments options for autoplot.forecast() to create completely different plots in with different visual appearance, I instead created a modular forecast geometry geom_forecast() which can be used to create any plot. I also created a forecast statistic, stat_forecast(), to also calculate forecasts by default much like geom_smooth() calculates a smooth line through the data8.\n\nlibrary(ggplot2)\nwineind |&gt; \n  ggplot(aes(x=x, y=y)) + \n  geom_line() + \n  geom_forecast(h = 48)\n\n\n\n\n\n\n\n\nAfter creating geom_forecast() ggplot2 extension, I then was able to vastly simplify the plot helper’s code by using this new forecast geometry. This approach of creating both plot helpers and ggplot2 extensions allows for both:\n\nA quick and simple plot helper function for looking at data in an analysis.\nA flexible and modular ggplot2 extension that is reusable to create many plots with different structure and style.\n\n\n\nIterated improvements in visualising uncertainty\n\nSeveral incremental design and implementation improvements have been made for visualising forecasts since first creating the plot helpers and grammar extensions described above. The fable package9 for forecasting still offers the same quick and easy autoplot() function for plotting forecasts:\n\nlibrary(tsibble)\nlibrary(fable)\nwine_tsbl &lt;- as_tsibble(wineind)\nwine_fbl &lt;- wine_tsbl |&gt; \n  model(ETS(value)) |&gt; \n  forecast(h = 48)\nwine_fbl |&gt; \n  autoplot(wine_tsbl)\n\n\n\n\n\n\n\n\nHowever the visualisation of forecasts is now built up with smaller and more modular components. In particular, the forecast object (a fable) is now rectangular and ready to use directly with ggplot2 and other tidy/rectangular functions.\n\nwine_fbl\n\n# A fable: 48 x 4 [1M]\n# Key:     .model [1]\n   .model        index             value  .mean\n   &lt;chr&gt;         &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;\n 1 ETS(value) 1994 Sep N(24739, 4756552) 24739.\n 2 ETS(value) 1994 Oct N(26972, 5674061) 26972.\n 3 ETS(value) 1994 Nov N(31771, 7901147) 31771.\n 4 ETS(value) 1994 Dec N(37060, 1.1e+07) 37060.\n 5 ETS(value) 1995 Jan N(16590, 2169674) 16590.\n 6 ETS(value) 1995 Feb N(21206, 3557682) 21206.\n 7 ETS(value) 1995 Mar N(23948, 4553053) 23948.\n 8 ETS(value) 1995 Apr N(25620, 5229329) 25620.\n 9 ETS(value) 1995 May N(24054, 4625845) 24054.\n10 ETS(value) 1995 Jun N(24662, 4880015) 24662.\n# i 38 more rows\n\n\nMost importantly is that the forecasts are represented with distributions (via the distributional package), which can be directly visualised with ggplot2 (via the ggdist package). The ggdist package provides several statistics (stat_*()) which allow you to use distributions in ggplot2 via the dist argument.\n\nlibrary(ggdist)\nwine_fbl |&gt;  \n  ggplot(aes(x = index)) + \n  # Past data\n  geom_line(aes(y = value), data = wine_tsbl) + \n  # Forecasts\n  stat_lineribbon(aes(dist = value, fill_ramp = after_stat(.width)), \n                  fill = \"steelblue\", colour = \"steelblue\",\n                  linewidth = 0.4, .width = c(0.8, 0.95)) +\n  scale_fill_ramp_continuous(limits = c(0.7, 1), range = c(1, 0))\n\n\n\n\n\n\n\n\nIt is more complicated to create this plot since it requires decisions than you might expect - colours, scales, sizing and more. However thinking about visualising distributions with modular grammar components also allows you to switch out geometries to visualise forecasts differently.\n\nhead(wine_fbl, 12) |&gt; \n  ggplot(aes(x = index)) + \n  # Past data\n  geom_line(aes(y = value), data = tail(wine_tsbl, 36)) + \n  # Forecasts\n  stat_halfeye(aes(dist = value), size = 0.5, \n               colour = \"steelblue\", fill = \"#82c3f2\") + \n  geom_line(aes(y = .mean), colour = \"steelblue\")\n\n\n\n\n\n\n\n\nVisualising distributions with ggdist are not limited to forecasting, and by designing extensions to be more modular (and consequently, do less) they can be reused more widely."
  },
  {
    "objectID": "blog/ggplot2-packages/index.html#conclusion",
    "href": "blog/ggplot2-packages/index.html#conclusion",
    "title": "Why I refuse to add style options to my ggplot2 plot functions",
    "section": "Conclusion",
    "text": "Conclusion\nWhile creating, learning and using grammar elements is harder than plot helper functions, they allow you to create more visually beautiful and analytically useful graphics. Good design of these grammar elements enables their reusability in making any style of plot for any purpose.\nPlot helpers can’t possibly create the perfect visualisation for your analysis - and they shouldn’t try to10. Attempts to make helper functions do more make them harder to learn and use11. To keep plot helper functions simple, I refuse to add extra options for changing the style of the plot even if it is easy to do so."
  },
  {
    "objectID": "blog/ggplot2-packages/index.html#footnotes",
    "href": "blog/ggplot2-packages/index.html#footnotes",
    "title": "Why I refuse to add style options to my ggplot2 plot functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe few times I have implemented these requests I later realise I shouldn’t have.↩︎\nI take semantic/pedantic issue with calling functions which use ggplot2 ‘extensions’, and think this causes some design confusion among developers and users. Both function types appear in the ggplot2 official ‘extensions’ gallery, and consequently it is common for all ggplot2-related packages to be called extensions.↩︎\nFor more details I highly recommend this 20 minute presentation - The Glamour of Graphics by William Chase↩︎\nThe Extending ggplot2 vignette is a good place to start learning the ggproto extension system.↩︎\nFurther than that, geom_bar() is a special case of geom_rect().↩︎\nThis is also why we don’t have a geom_piechart(), since it would involve both a bar geometry and polar coordinate elements.↩︎\nautoplot() is a ggplot2 function for creating plot helpers for a specific object class.↩︎\nAs mentioned earlier with geom_histogram() I think more separation between geometry and statistic is a better design than what I previously created with geom_forecast().↩︎\nThe tidy/rectangular successor to the forecast package.↩︎\nI’m pleased to see others take on this belief in these great Stack Overflow answers: https://stackoverflow.com/questions/78512507/stop-fabletools-autoplot-from-using-color-aesthetic-for-models-when-faceting↩︎\nThis is especially evident in Cynthia Huang’s comparison of three calendar plot helper variants↩︎"
  },
  {
    "objectID": "blog/hexwall/index.html",
    "href": "blog/hexwall/index.html",
    "title": "Arranging hex stickers in R",
    "section": "",
    "text": "Neatly aligning hex stickers for presentations or reports is a tedious process. It is repetitive, and very easy to get wrong. Despite the physical hexagons having a standardised shape and size, digital hexagons appear in many forms. With variation in file type, resolution, transparency and layouts, working with hexagon files can prove challenging.\nThe code in this blog is bundled into the hexwall R script (available at mitchelloharawild/hexwall), making it simpler to arrange your stickers.\nMy need for this functionality began with the creation of a hex sticker feature wall at useR! 2018. This involved arranging approximately 200 stickers in the shape of Australia, which you can read about in this blog post.\nIn that project we collected the stickers via email, but for this blog we will use the RStudio hex-stickers from their GitHub repository (rstudio/hex-stickers).\nI’ve picked out a few hex stickers from this repository and placed the png images from the PNG folder in a hex-stickers folder."
  },
  {
    "objectID": "blog/hexwall/index.html#preparation",
    "href": "blog/hexwall/index.html#preparation",
    "title": "Arranging hex stickers in R",
    "section": "Preparation",
    "text": "Preparation\nBefore we start aligning the hexagons, we first need to convert them to a common size and format. In most cases, the conversion can be done automatically with the ROpenSci magick package.\nAll images can be read with image_read, however better quality for svg and png formats can be obtained using their specific reading functions. As many stickers had white backgrounds (especially pdf format images), I first convert white to transparent, and then use image_trim to automatically crop the images.\n\nlibrary(magick)\nlibrary(purrr)\n\nsticker_files &lt;- list.files(\"hex-stickers\", full.names = TRUE)\nstickers &lt;- sticker_files %&gt;% \n  map(compose(image_read, ~ image_transparent(., \"white\"), image_trim, .dir = \"forward\")) %&gt;%\n  set_names(basename(sticker_files))\n\nUnfortunately some stickers can not be fixed automatically, as they were either too low resolution or incorrectly shaped. These cases are easily identifiable for manual fixing from the image information provided by image_info().\n\n# Desired sticker resolution in pixels\nsticker_width &lt;- 121\n\n# Scale all stickers to the desired pixel width\nstickers &lt;- stickers %&gt;%\n  map(image_scale, sticker_width)\n  \n# Identify low resolution stickers\nstickers %&gt;%\n  map_lgl(~ with(\n    image_info(.x),\n    width &lt; (sticker_width-1)/2 && format != \"svg\"\n  ))\n\n#&gt;   forcats.png   ggplot2.png      pipe.png     purrr.png tidyverse.png \n#&gt;         FALSE         FALSE         FALSE         FALSE         FALSE\n\n# Identify incorrect shapes / proportions (tolerance of +-2 height)\nstickers %&gt;%\n  map_lgl(~ with(\n    image_info(.x),\n    height &lt; (median(height)-2) | height &gt; (median(height) + 2)\n  ))\n\n#&gt;   forcats.png   ggplot2.png      pipe.png     purrr.png tidyverse.png \n#&gt;         FALSE         FALSE         FALSE         FALSE         FALSE\n\n\nAs some stickers may have slightly different proportions (wihin tolerances above), we first force the stickers to have identical dimensions for alignment.\n\n# Extract correct sticker height (this could also be calculated directly from width)\nsticker_height &lt;- stickers %&gt;%\n  map(image_info) %&gt;%\n  map_dbl(\"height\") %&gt;%\n  median\n\n# Coerce sticker dimensions\nstickers &lt;- stickers %&gt;%\n  map(image_resize, paste0(sticker_width, \"x\", sticker_height, \"!\"))\n\nstickers[[\"tidyverse.png\"]]"
  },
  {
    "objectID": "blog/hexwall/index.html#alignment",
    "href": "blog/hexwall/index.html#alignment",
    "title": "Arranging hex stickers in R",
    "section": "Alignment",
    "text": "Alignment\nUsing magick, it is relatively trivial to align hexagons into rows using magick’s image_append() functionality. The total hexagons in each row alternates between odd and even numbers.\n\nsticker_row_size &lt;- 3\n# Calculate row sizes\nsticker_col_size &lt;- ceiling(length(stickers)/(sticker_row_size-0.5))\nrow_lens &lt;- rep(c(sticker_row_size,sticker_row_size-1), length.out=sticker_col_size)\nrow_lens[length(row_lens)] &lt;- row_lens[length(row_lens)]  - (length(stickers) - sum(row_lens))\n\nsticker_rows &lt;- map2(row_lens, cumsum(row_lens),\n                     ~ seq(.y-.x+1, by = 1, length.out = .x)) %&gt;%\n  map(~ stickers[.x] %&gt;%\n        invoke(c, .) %&gt;%\n        image_append)\n\n#&gt; Warning: `invoke()` was deprecated in purrr 1.0.0.\n#&gt; i Please use `exec()` instead.\n\nsticker_rows[[1]]\n\n\n\n\n\n\n\n\nTo simplify the placement of these sticker rows, we can first create a white canvas for us to place sticker rows on. The width is simple to calculate, but the height is slightly more complex. The extra height of each row is the height of the left side of the hexagon (approximately sticker_height/1.33526), then to add the angled parts of the hexagon for the top and bottom, we add the height of one whole sticker.\n\n# Add stickers to canvas\ncanvas &lt;- image_blank(sticker_row_size*sticker_width, \n                      sticker_height + (sticker_col_size-1)*sticker_height/1.33526,\n                      \"white\")\n\nWith our canvas and list of sticker_rows, it is time to add them to the final image using image_composite. The offset of the rows is calculated from the current row number (..3), which is used to add the current row (..2) to the canvas (..1).\n\nreduce2(sticker_rows, seq_along(sticker_rows), \n        ~ image_composite(\n          ..1, ..2,\n          offset = paste0(\"+\", ((..3-1)%%2)*sticker_width/2,\n                          \"+\", round((..3-1)*sticker_height/1.33526))\n        ),\n        .init = canvas)"
  },
  {
    "objectID": "blog/hexwall/index.html#hexwall",
    "href": "blog/hexwall/index.html#hexwall",
    "title": "Arranging hex stickers in R",
    "section": "hexwall",
    "text": "hexwall\nNow that you know how it works, try it out on your own hexagon stickers. You can use the code above, or the script available on Github (mitchelloharawild/hexwall). To use the hexwall script on this example, you would use the hexwall function like this:\n\nsource(\"hexwall.R\")\nhexwall(\"path\", sticker_row_size = 9, sticker_width = 121)"
  }
]